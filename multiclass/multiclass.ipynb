{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading labels ...\n",
      "Number of labels 11\n",
      "Found 1650 images belonging to 11 classes.\n",
      "Found 550 images belonging to 11 classes.\n",
      "{0: 'Bread', 1: 'Dairy product', 2: 'Dessert', 3: 'Egg', 4: 'Fried food', 5: 'Meat', 6: 'Noodles-Pasta', 7: 'Rice', 8: 'Seafood', 9: 'Soup', 10: 'Vegetable-Fruit'}\n",
      "[INFO] training network...\n",
      "Epoch 1/100\n",
      "104/104 [==============================] - 974s 9s/step - loss: 2.5507 - categorical_accuracy: 0.0925 - val_loss: 2.4439 - val_categorical_accuracy: 0.1091\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.44386, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 2/100\n",
      "104/104 [==============================] - 989s 9s/step - loss: 2.4774 - categorical_accuracy: 0.1052 - val_loss: 2.3730 - val_categorical_accuracy: 0.1455\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.44386 to 2.37305, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 3/100\n",
      "104/104 [==============================] - 1043s 10s/step - loss: 2.4138 - categorical_accuracy: 0.1257 - val_loss: 2.3231 - val_categorical_accuracy: 0.1945\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.37305 to 2.32313, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 4/100\n",
      "104/104 [==============================] - 1007s 10s/step - loss: 2.3440 - categorical_accuracy: 0.1618 - val_loss: 2.2763 - val_categorical_accuracy: 0.2455\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.32313 to 2.27634, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 5/100\n",
      "104/104 [==============================] - 1060s 10s/step - loss: 2.2661 - categorical_accuracy: 0.2243 - val_loss: 2.2316 - val_categorical_accuracy: 0.3055\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.27634 to 2.23159, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 6/100\n",
      "104/104 [==============================] - 1128s 11s/step - loss: 2.2699 - categorical_accuracy: 0.1918 - val_loss: 2.1869 - val_categorical_accuracy: 0.3509\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.23159 to 2.18687, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 7/100\n",
      "104/104 [==============================] - 1102s 11s/step - loss: 2.2085 - categorical_accuracy: 0.2449 - val_loss: 2.1396 - val_categorical_accuracy: 0.4036\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.18687 to 2.13960, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 8/100\n",
      "104/104 [==============================] - 1037s 10s/step - loss: 2.2045 - categorical_accuracy: 0.2243 - val_loss: 2.0913 - val_categorical_accuracy: 0.4436\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.13960 to 2.09132, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 9/100\n",
      "104/104 [==============================] - 966s 9s/step - loss: 2.1237 - categorical_accuracy: 0.3041 - val_loss: 2.0426 - val_categorical_accuracy: 0.4727\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.09132 to 2.04264, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 10/100\n",
      "104/104 [==============================] - 947s 9s/step - loss: 2.0584 - categorical_accuracy: 0.3705 - val_loss: 1.9904 - val_categorical_accuracy: 0.5127\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.04264 to 1.99045, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 11/100\n",
      "104/104 [==============================] - 946s 9s/step - loss: 2.0125 - categorical_accuracy: 0.3952 - val_loss: 1.9380 - val_categorical_accuracy: 0.5418\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.99045 to 1.93801, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 12/100\n",
      "104/104 [==============================] - 957s 9s/step - loss: 1.9777 - categorical_accuracy: 0.4016 - val_loss: 1.8844 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.93801 to 1.88437, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 13/100\n",
      "104/104 [==============================] - 946s 9s/step - loss: 1.9616 - categorical_accuracy: 0.4141 - val_loss: 1.8306 - val_categorical_accuracy: 0.5836\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.88437 to 1.83062, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 14/100\n",
      "104/104 [==============================] - 946s 9s/step - loss: 1.8849 - categorical_accuracy: 0.4645 - val_loss: 1.7750 - val_categorical_accuracy: 0.5891\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.83062 to 1.77504, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 15/100\n",
      "104/104 [==============================] - 962s 9s/step - loss: 1.8301 - categorical_accuracy: 0.4793 - val_loss: 1.7200 - val_categorical_accuracy: 0.6036\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.77504 to 1.71996, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 16/100\n",
      "104/104 [==============================] - 970s 9s/step - loss: 1.7850 - categorical_accuracy: 0.5007 - val_loss: 1.6665 - val_categorical_accuracy: 0.6236\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.71996 to 1.66651, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 17/100\n",
      "104/104 [==============================] - 1080s 10s/step - loss: 1.6963 - categorical_accuracy: 0.5464 - val_loss: 1.6127 - val_categorical_accuracy: 0.6345\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.66651 to 1.61273, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 18/100\n",
      "104/104 [==============================] - 1093s 11s/step - loss: 1.6288 - categorical_accuracy: 0.5680 - val_loss: 1.5619 - val_categorical_accuracy: 0.6364\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.61273 to 1.56185, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 19/100\n",
      "104/104 [==============================] - 1095s 11s/step - loss: 1.6010 - categorical_accuracy: 0.5726 - val_loss: 1.5143 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.56185 to 1.51433, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 20/100\n",
      "104/104 [==============================] - 1073s 10s/step - loss: 1.5628 - categorical_accuracy: 0.5869 - val_loss: 1.4667 - val_categorical_accuracy: 0.6473\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.51433 to 1.46674, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 21/100\n",
      "104/104 [==============================] - 1065s 10s/step - loss: 1.5307 - categorical_accuracy: 0.5869 - val_loss: 1.4226 - val_categorical_accuracy: 0.6582\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.46674 to 1.42256, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 22/100\n",
      "104/104 [==============================] - 1100s 11s/step - loss: 1.4556 - categorical_accuracy: 0.5989 - val_loss: 1.3808 - val_categorical_accuracy: 0.6618\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.42256 to 1.38080, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 23/100\n",
      "104/104 [==============================] - 1091s 10s/step - loss: 1.3931 - categorical_accuracy: 0.6197 - val_loss: 1.3379 - val_categorical_accuracy: 0.6709\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.38080 to 1.33788, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 24/100\n",
      "104/104 [==============================] - 998s 10s/step - loss: 1.4070 - categorical_accuracy: 0.6152 - val_loss: 1.2999 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.33788 to 1.29988, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 25/100\n",
      "104/104 [==============================] - 973s 9s/step - loss: 1.3458 - categorical_accuracy: 0.6386 - val_loss: 1.2622 - val_categorical_accuracy: 0.6836\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.29988 to 1.26225, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 26/100\n",
      "104/104 [==============================] - 1018s 10s/step - loss: 1.3165 - categorical_accuracy: 0.6459 - val_loss: 1.2268 - val_categorical_accuracy: 0.6909\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.26225 to 1.22678, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 27/100\n",
      "104/104 [==============================] - 1311s 13s/step - loss: 1.2516 - categorical_accuracy: 0.6658 - val_loss: 1.1923 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.22678 to 1.19235, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 28/100\n",
      "104/104 [==============================] - 1273s 12s/step - loss: 1.1875 - categorical_accuracy: 0.6892 - val_loss: 1.1569 - val_categorical_accuracy: 0.7018\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.19235 to 1.15687, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 997s 10s/step - loss: 1.1915 - categorical_accuracy: 0.6737 - val_loss: 1.1238 - val_categorical_accuracy: 0.7073\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.15687 to 1.12376, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 30/100\n",
      "104/104 [==============================] - 1073s 10s/step - loss: 1.1771 - categorical_accuracy: 0.6770 - val_loss: 1.0901 - val_categorical_accuracy: 0.7109\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.12376 to 1.09007, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 31/100\n",
      "104/104 [==============================] - 978s 9s/step - loss: 1.1377 - categorical_accuracy: 0.6830 - val_loss: 1.0609 - val_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.09007 to 1.06095, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 32/100\n",
      "104/104 [==============================] - 949s 9s/step - loss: 1.0976 - categorical_accuracy: 0.7038 - val_loss: 1.0334 - val_categorical_accuracy: 0.7309\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.06095 to 1.03342, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 33/100\n",
      "104/104 [==============================] - 947s 9s/step - loss: 1.0344 - categorical_accuracy: 0.7116 - val_loss: 1.0077 - val_categorical_accuracy: 0.7418\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.03342 to 1.00775, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 34/100\n",
      "104/104 [==============================] - 952s 9s/step - loss: 1.0316 - categorical_accuracy: 0.7265 - val_loss: 0.9819 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.00775 to 0.98193, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 35/100\n",
      "104/104 [==============================] - 955s 9s/step - loss: 1.0220 - categorical_accuracy: 0.7290 - val_loss: 0.9563 - val_categorical_accuracy: 0.7473\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.98193 to 0.95630, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 36/100\n",
      "104/104 [==============================] - 952s 9s/step - loss: 0.9718 - categorical_accuracy: 0.7258 - val_loss: 0.9326 - val_categorical_accuracy: 0.7564\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.95630 to 0.93257, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 37/100\n",
      "104/104 [==============================] - 950s 9s/step - loss: 0.9593 - categorical_accuracy: 0.7397 - val_loss: 0.9070 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.93257 to 0.90696, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 38/100\n",
      "104/104 [==============================] - 946s 9s/step - loss: 0.9634 - categorical_accuracy: 0.7160 - val_loss: 0.8823 - val_categorical_accuracy: 0.7673\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.90696 to 0.88230, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 39/100\n",
      "104/104 [==============================] - 950s 9s/step - loss: 0.9293 - categorical_accuracy: 0.7499 - val_loss: 0.8639 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.88230 to 0.86392, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 40/100\n",
      "104/104 [==============================] - 967s 9s/step - loss: 0.8845 - categorical_accuracy: 0.7733 - val_loss: 0.8433 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.86392 to 0.84332, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 41/100\n",
      "104/104 [==============================] - 1106s 11s/step - loss: 0.8965 - categorical_accuracy: 0.7480 - val_loss: 0.8222 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.84332 to 0.82221, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 42/100\n",
      "104/104 [==============================] - 977s 9s/step - loss: 0.8613 - categorical_accuracy: 0.7595 - val_loss: 0.8057 - val_categorical_accuracy: 0.7818\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.82221 to 0.80568, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 43/100\n",
      "104/104 [==============================] - 961s 9s/step - loss: 0.8742 - categorical_accuracy: 0.7488 - val_loss: 0.7882 - val_categorical_accuracy: 0.7891\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.80568 to 0.78824, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 44/100\n",
      "104/104 [==============================] - 978s 9s/step - loss: 0.8046 - categorical_accuracy: 0.7858 - val_loss: 0.7695 - val_categorical_accuracy: 0.7909\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.78824 to 0.76946, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 45/100\n",
      "104/104 [==============================] - 1078s 10s/step - loss: 0.8049 - categorical_accuracy: 0.7664 - val_loss: 0.7530 - val_categorical_accuracy: 0.7873\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.76946 to 0.75299, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 46/100\n",
      "104/104 [==============================] - 988s 9s/step - loss: 0.7712 - categorical_accuracy: 0.7726 - val_loss: 0.7367 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.75299 to 0.73672, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 47/100\n",
      "104/104 [==============================] - 961s 9s/step - loss: 0.7408 - categorical_accuracy: 0.7932 - val_loss: 0.7226 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.73672 to 0.72259, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 48/100\n",
      "104/104 [==============================] - 951s 9s/step - loss: 0.7342 - categorical_accuracy: 0.7963 - val_loss: 0.7069 - val_categorical_accuracy: 0.8036\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.72259 to 0.70690, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 49/100\n",
      "104/104 [==============================] - 949s 9s/step - loss: 0.7402 - categorical_accuracy: 0.7964 - val_loss: 0.6925 - val_categorical_accuracy: 0.8127\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.70690 to 0.69247, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 50/100\n",
      "104/104 [==============================] - 949s 9s/step - loss: 0.6969 - categorical_accuracy: 0.8089 - val_loss: 0.6823 - val_categorical_accuracy: 0.8109\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.69247 to 0.68230, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 51/100\n",
      "104/104 [==============================] - 945s 9s/step - loss: 0.7031 - categorical_accuracy: 0.8025 - val_loss: 0.6672 - val_categorical_accuracy: 0.8218\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.68230 to 0.66719, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 52/100\n",
      "104/104 [==============================] - 944s 9s/step - loss: 0.6759 - categorical_accuracy: 0.8070 - val_loss: 0.6549 - val_categorical_accuracy: 0.8182\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.66719 to 0.65491, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 53/100\n",
      "104/104 [==============================] - 940s 9s/step - loss: 0.6874 - categorical_accuracy: 0.8119 - val_loss: 0.6421 - val_categorical_accuracy: 0.8236\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.65491 to 0.64207, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 54/100\n",
      "104/104 [==============================] - 940s 9s/step - loss: 0.6678 - categorical_accuracy: 0.7955 - val_loss: 0.6335 - val_categorical_accuracy: 0.8255\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.64207 to 0.63352, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 55/100\n",
      "104/104 [==============================] - 941s 9s/step - loss: 0.6391 - categorical_accuracy: 0.8194 - val_loss: 0.6224 - val_categorical_accuracy: 0.8273\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.63352 to 0.62244, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 56/100\n",
      "104/104 [==============================] - 939s 9s/step - loss: 0.6408 - categorical_accuracy: 0.8150 - val_loss: 0.6123 - val_categorical_accuracy: 0.8255\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.62244 to 0.61233, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 57/100\n",
      "104/104 [==============================] - 942s 9s/step - loss: 0.6602 - categorical_accuracy: 0.8116 - val_loss: 0.6048 - val_categorical_accuracy: 0.8291\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.61233 to 0.60480, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 941s 9s/step - loss: 0.6151 - categorical_accuracy: 0.8352 - val_loss: 0.5967 - val_categorical_accuracy: 0.8327\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.60480 to 0.59673, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 59/100\n",
      "104/104 [==============================] - 940s 9s/step - loss: 0.5827 - categorical_accuracy: 0.8379 - val_loss: 0.5860 - val_categorical_accuracy: 0.8327\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.59673 to 0.58599, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 60/100\n",
      "104/104 [==============================] - 941s 9s/step - loss: 0.6332 - categorical_accuracy: 0.8114 - val_loss: 0.5772 - val_categorical_accuracy: 0.8309\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.58599 to 0.57722, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 61/100\n",
      "104/104 [==============================] - 944s 9s/step - loss: 0.5972 - categorical_accuracy: 0.8122 - val_loss: 0.5692 - val_categorical_accuracy: 0.8364\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.57722 to 0.56922, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 62/100\n",
      "104/104 [==============================] - 940s 9s/step - loss: 0.5839 - categorical_accuracy: 0.8360 - val_loss: 0.5611 - val_categorical_accuracy: 0.8345\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.56922 to 0.56111, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 63/100\n",
      "104/104 [==============================] - 941s 9s/step - loss: 0.5520 - categorical_accuracy: 0.8484 - val_loss: 0.5538 - val_categorical_accuracy: 0.8364\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.56111 to 0.55378, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 64/100\n",
      "104/104 [==============================] - 942s 9s/step - loss: 0.5240 - categorical_accuracy: 0.8371 - val_loss: 0.5462 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.55378 to 0.54616, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 65/100\n",
      "104/104 [==============================] - 940s 9s/step - loss: 0.5288 - categorical_accuracy: 0.8397 - val_loss: 0.5426 - val_categorical_accuracy: 0.8418\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.54616 to 0.54259, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 66/100\n",
      "104/104 [==============================] - 928s 9s/step - loss: 0.5271 - categorical_accuracy: 0.8406 - val_loss: 0.5345 - val_categorical_accuracy: 0.8473\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.54259 to 0.53445, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 67/100\n",
      "104/104 [==============================] - 942s 9s/step - loss: 0.5077 - categorical_accuracy: 0.8728 - val_loss: 0.5276 - val_categorical_accuracy: 0.8473\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.53445 to 0.52758, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 68/100\n",
      "104/104 [==============================] - 940s 9s/step - loss: 0.5090 - categorical_accuracy: 0.8491 - val_loss: 0.5183 - val_categorical_accuracy: 0.8436\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.52758 to 0.51832, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 69/100\n",
      "104/104 [==============================] - 997s 10s/step - loss: 0.5281 - categorical_accuracy: 0.8485 - val_loss: 0.5156 - val_categorical_accuracy: 0.8473\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.51832 to 0.51556, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 70/100\n",
      "104/104 [==============================] - 952s 9s/step - loss: 0.5033 - categorical_accuracy: 0.8638 - val_loss: 0.5129 - val_categorical_accuracy: 0.8491\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.51556 to 0.51289, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 71/100\n",
      "104/104 [==============================] - 956s 9s/step - loss: 0.4475 - categorical_accuracy: 0.8785 - val_loss: 0.5076 - val_categorical_accuracy: 0.8491\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.51289 to 0.50757, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 72/100\n",
      "104/104 [==============================] - 959s 9s/step - loss: 0.4512 - categorical_accuracy: 0.8746 - val_loss: 0.5010 - val_categorical_accuracy: 0.8527\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.50757 to 0.50100, saving model to models\\inceptionv3_multiclass_best.h5\n",
      "Epoch 73/100\n",
      "  3/104 [..............................] - ETA: 14:59 - loss: 0.6670 - categorical_accuracy: 0.7917"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ad79f5e57bff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mTYPE_CLASSIFIER\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'multiclass'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_valid_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultilabel_train_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultilabel_validation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_valid_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import applications\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import keras.backend as K\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras_tqdm import TQDMCallback\n",
    "\n",
    "def build_labels_dict(dataset_path, recipe_food_map_path):\n",
    "    print(\"[INFO] loading labels ...\")\n",
    "    recipe_food_map = pd.read_csv(recipe_food_map_path)\n",
    "    labels_list = list(recipe_food_map['category'].unique())\n",
    "    food=list(recipe_food_map['photo_id'])\n",
    "    cat=list(recipe_food_map['category'])\n",
    "    recipe_food_dict = {food[i]: cat[i] for i in range(len(cat))}\n",
    "    labels_list.sort()\n",
    "    return recipe_food_dict, labels_list\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    MODELS_DIR = 'models'\n",
    "    DATA_DIR = ''\n",
    "    RECIPE_FOOD_MAP = os.path.join(DATA_DIR, 'sampledata.csv')\n",
    "    TYPE_CLASSIFIER = 'multiclass'\n",
    "    TRAIN_DIR = os.path.join(DATA_DIR, 'sample_training')\n",
    "    VALID_DIR = os.path.join(DATA_DIR, 'sample_testing')\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 100\n",
    "    INIT_LR = 1e-6\n",
    "    IMG_WIDTH, IMG_HEIGHT = 299, 299  \n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, IMG_WIDTH, IMG_HEIGHT)\n",
    "    else:\n",
    "        input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "\n",
    "    num_train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])\n",
    "    num_valid_samples = sum([len(files) for r, d, files in os.walk(VALID_DIR)])\n",
    "\n",
    "    num_train_steps = num_train_samples // BATCH_SIZE + 1\n",
    "    num_valid_steps = num_valid_samples // BATCH_SIZE + 1\n",
    "\n",
    "    recipe_food_dict, labels_list = build_labels_dict(DATA_DIR, RECIPE_FOOD_MAP)\n",
    "    print (\"Number of labels {}\".format(len(labels_list)))\n",
    "\n",
    "    # construct the image generator for data augmentation\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       rotation_range=25,\n",
    "                                       width_shift_range=0.1,\n",
    "                                       height_shift_range=0.1,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       fill_mode=\"nearest\")\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = train_datagen.flow_from_directory(TRAIN_DIR, target_size=(IMG_WIDTH, IMG_HEIGHT), batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "    validation_generator = test_datagen.flow_from_directory(VALID_DIR, target_size=(IMG_WIDTH, IMG_HEIGHT), batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "\n",
    "    label_map = (train_generator.class_indices)\n",
    "    label_map = dict((v, k) for k, v in label_map.items())\n",
    "    print(label_map)\n",
    "\n",
    "    # create the base pre-trained model\n",
    "    base_model = applications.inception_v3.InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # compile the model using binary cross-entropy rather than categorical cross-entropy -- th==may seem counterintuitive for\n",
    "    # multi-label classification, but keep in mind that the goal here ==to treat each output label as an independent Bernoulli distribution:\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=INIT_LR), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(os.path.join(MODELS_DIR, 'inceptionv3_' + TYPE_CLASSIFIER + '_best.h5'), verbose=1, save_best_only=True)\n",
    "\n",
    "    # train the network\n",
    "    print(\"[INFO] training network...\")\n",
    "    history = model.fit(train_generator, steps_per_epoch=num_train_steps, epochs=EPOCHS, verbose=1, callbacks=[early_stopping, checkpointer], validation_data=validation_generator, validation_steps=num_valid_steps)\n",
    "    model.save(os.path.join(MODELS_DIR, 'inceptionv3_' + TYPE_CLASSIFIER + '_final.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(MODELS_DIR, 'inceptionv3_' + TYPE_CLASSIFIER + '_final.h5'))\n",
    "show_acc_history(history)\n",
    "show_loss_history(history)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
